Use Autoencoder to implement anomaly detection. Build the model by using: 
a. Import required libraries b. Upload / access the dataset 
c. Encoder converts it into latent representation 
d. Decoder networks convert it back to the original input 
e. Compile the models with Optimizer, Loss, and Evaluation Metrics

1. #importing libraries and dataset
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.losses import MeanSquaredLogarithmicError

PATH_TO_DATA = 'http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'
data = pd.read_csv(PATH_TO_DATA, header=None)
data.head()

2. #finding shape of the dataset
data.shape

3. #splitting training and testing dataset
features = data.drop(140, axis=1)
target = data[140]
x_train, x_test, y_train, y_test = train_test_split(
    features, target, test_size=0.2, stratify=target
)
train_index = y_train[y_train == 1].index
train_data = x_train.loc[train_index]

4. #scaling the data using MinMaxScaler
min_max_scaler = MinMaxScaler(feature_range=(0, 1))
x_train_scaled = min_max_scaler.fit_transform(train_data.copy())
x_test_scaled = min_max_scaler.transform(x_test.copy())

5. #creating autoencoder subclass by extending Model class from keras
class AutoEncoder(Model):
  def __init__(self, output_units, ldim=8):
    super().__init__()
    self.encoder = Sequential([
      Dense(64, activation='relu'),
      Dropout(0.1),
      Dense(32, activation='relu'),
      Dropout(0.1),
      Dense(16, activation='relu'),
      Dropout(0.1),
      Dense(ldim, activation='relu')
    ])
    self.decoder = Sequential([
      Dense(16, activation='relu'),
      Dropout(0.1),
      Dense(32, activation='relu'),
      Dropout(0.1),
      Dense(64, activation='relu'),
      Dropout(0.1),
      Dense(output_units, activation='sigmoid')
    ])
  
  def call(self, inputs):
    encoded = self.encoder(inputs)
    decoded = self.decoder(encoded)
    return decoded

6. #model configuration
model = AutoEncoder(output_units=x_train_scaled.shape[1])
model.compile(loss='msle', metrics=['mse'], optimizer='adam')
epochs = 20

history = model.fit(
    x_train_scaled,
    x_train_scaled,
    epochs=epochs,
    batch_size=512,
    validation_data=(x_test_scaled, x_test_scaled)
)

7. plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('Epochs')
plt.ylabel('MSLE Loss')
plt.legend(['loss', 'val_loss'])
plt.show()

8. #finding threshold for anomaly and doing predictions
def find_threshold(model, x_train_scaled):
  reconstructions = model.predict(x_train_scaled)
  reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)
  threshold = np.mean(reconstruction_errors.numpy()) \
   + np.std(reconstruction_errors.numpy())
  return threshold

def get_predictions(model, x_test_scaled, threshold):
  predictions = model.predict(x_test_scaled)
  errors = tf.keras.losses.msle(predictions, x_test_scaled)
  anomaly_mask = pd.Series(errors) > threshold
  preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)
  return preds

threshold = find_threshold(model, x_train_scaled)
print(f"Threshold: {threshold}")

9. #getting accuracy score
predictions = get_predictions(model, x_test_scaled, threshold)
accuracy_score(predictions, y_test)


Stage 1: Import Required Libraries
python
Copy code
# Importing libraries and dataset
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.losses import MeanSquaredLogarithmicError

# Load ECG dataset
PATH_TO_DATA = 'http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'
data = pd.read_csv(PATH_TO_DATA, header=None)
data.head()
Here, we import the necessary libraries and load the ECG dataset. This dataset will be used for anomaly detection with the Autoencoder.

Stage 2: Finding the Shape of the Dataset
python
Copy code
# Finding the shape of the dataset
data.shape
This command helps confirm the structure of the dataset before processing it further.

Stage 3: Splitting Training and Testing Dataset
python
Copy code
# Splitting training and testing dataset
features = data.drop(140, axis=1)  # Drop the target column
target = data[140]  # Target column

x_train, x_test, y_train, y_test = train_test_split(
    features, target, test_size=0.2, stratify=target
)

train_index = y_train[y_train == 1].index  # Only select normal data
train_data = x_train.loc[train_index]
Here, we split the data into training and testing sets, focusing on the "normal" class (labeled as 1) for training the Autoencoder.

Stage 4: Scaling the Data using MinMaxScaler
python
Copy code
# Scaling the data using MinMaxScaler
min_max_scaler = MinMaxScaler(feature_range=(0, 1))
x_train_scaled = min_max_scaler.fit_transform(train_data.copy())  # Scale training data
x_test_scaled = min_max_scaler.transform(x_test.copy())  # Scale test data
MinMaxScaler ensures that all the input features are normalized between 0 and 1, which is important for training neural networks effectively.

Stage 5: Creating the Autoencoder Model
python
Copy code
# Creating Autoencoder subclass by extending Model class from Keras
class AutoEncoder(Model):
    def __init__(self, output_units, ldim=8):
        super().__init__()
        self.encoder = Sequential([
            Dense(64, activation='relu'),
            Dropout(0.1),
            Dense(32, activation='relu'),
            Dropout(0.1),
            Dense(16, activation='relu'),
            Dropout(0.1),
            Dense(ldim, activation='relu')  # Latent representation layer
        ])
        self.decoder = Sequential([
            Dense(16, activation='relu'),
            Dropout(0.1),
            Dense(32, activation='relu'),
            Dropout(0.1),
            Dense(64, activation='relu'),
            Dropout(0.1),
            Dense(output_units, activation='sigmoid')  # Output layer to reconstruct the input
        ])
  
    def call(self, inputs):
        encoded = self.encoder(inputs)
        decoded = self.decoder(encoded)
        return decoded
This class defines the Autoencoder model with an encoder and decoder. The encoder reduces the input data to a lower-dimensional latent representation, and the decoder reconstructs the input from that latent space.

Stage 6: Model Configuration and Training
python
Copy code
# Model configuration
model = AutoEncoder(output_units=x_train_scaled.shape[1])
model.compile(loss='msle', metrics=['mse'], optimizer='adam')
epochs = 20

history = model.fit(
    x_train_scaled,
    x_train_scaled,
    epochs=epochs,
    batch_size=512,
    validation_data=(x_test_scaled, x_test_scaled)
)
The model is compiled using the Adam optimizer, Mean Squared Logarithmic Error (MSLE) loss, and Mean Squared Error (MSE) as a metric. The Autoencoder is trained for 20 epochs on the scaled training data.

Stage 7: Plotting Loss Curve
python
Copy code
# Plotting loss curves
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('Epochs')
plt.ylabel('MSLE Loss')
plt.legend(['loss', 'val_loss'])
plt.show()
This plot helps visualize the model's loss during training and validation, ensuring that the model is learning effectively.

Stage 8: Finding the Threshold for Anomaly and Making Predictions
python
Copy code
# Finding threshold for anomaly and doing predictions
def find_threshold(model, x_train_scaled):
    reconstructions = model.predict(x_train_scaled)
    reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)
    threshold = np.mean(reconstruction_errors.numpy()) + np.std(reconstruction_errors.numpy())
    return threshold

def get_predictions(model, x_test_scaled, threshold):
    predictions = model.predict(x_test_scaled)
    errors = tf.keras.losses.msle(predictions, x_test_scaled)
    anomaly_mask = pd.Series(errors) > threshold  # Anomalies have higher reconstruction errors
    preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)  # 1.0: Normal, 0.0: Anomaly
    return preds

threshold = find_threshold(model, x_train_scaled)
print(f"Threshold: {threshold}")
This function calculates the reconstruction errors on the training set and sets the threshold as the mean plus one standard deviation of those errors. This threshold is used to detect anomalies in the test set.

Stage 9: Getting Accuracy Score
python
Copy code
# Getting accuracy score
predictions = get_predictions(model, x_test_scaled, threshold)
accuracy = accuracy_score(predictions, y_test)
print(f"Accuracy: {accuracy}")
Finally, we use the threshold to make predictions on the test set and compute the accuracy score by comparing the predicted anomalies with the true labels.

Summary:
This process defines an Autoencoder model for anomaly detection in ECG data. By reconstructing normal data, the model learns to identify outliers based on reconstruction error. The approach relies on detecting anomalies using the reconstruction error and comparing it against a threshold.